{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1ca468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85464dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "test_dataset = load_from_disk(\"../data/processed/test_dataset\")\n",
    "print(f\"Test examples: {len(test_dataset)}\")\n",
    "print(\"Sample test example:\", test_dataset[0])\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_path = \"./models/deepseek-quiz-flashcard-generator\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fc270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_quiz_flashcard(input_text, max_length=512):\n",
    "    \"\"\"Generate quiz/flashcard content using the fine-tuned model\"\"\"\n",
    "    prompt = f\"<|im_start|>user\\n{input_text}\\n<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate with appropriate parameters\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_length=max_length,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    \n",
    "    # Extract only the assistant's response\n",
    "    response = generated_text.split(\"<|im_start|>assistant\\n\")[-1].split(\"<|im_end|>\")[0].strip()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6242cbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_quiz_flashcard(input_text, max_length=512):\n",
    "    \"\"\"Generate quiz/flashcard content using the fine-tuned model\"\"\"\n",
    "    prompt = f\"<|im_start|>user\\n{input_text}\\n<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate with appropriate parameters\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_length=max_length,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    \n",
    "    # Extract only the assistant's response\n",
    "    response = generated_text.split(\"<|im_start|>assistant\\n\")[-1].split(\"<|im_end|>\")[0].strip()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91906fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize metrics calculators\n",
    "rouge_scorer_obj = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Run evaluation on test set\n",
    "results = []\n",
    "\n",
    "for i, example in enumerate(tqdm(test_dataset)):\n",
    "    input_text = example[\"content\"]  # Adjust field name if needed\n",
    "    reference = example[\"quiz_flashcard\"]  # Adjust field name if needed\n",
    "    \n",
    "    # Generate quiz/flashcard\n",
    "    try:\n",
    "        generated = generate_quiz_flashcard(input_text)\n",
    "        \n",
    "        # Calculate ROUGE scores\n",
    "        rouge_scores = rouge_scorer_obj.score(reference, generated)\n",
    "        \n",
    "        # Calculate BLEU score\n",
    "        reference_tokens = nltk.word_tokenize(reference.lower())\n",
    "        generated_tokens = nltk.word_tokenize(generated.lower())\n",
    "        bleu_score = sentence_bleu([reference_tokens], generated_tokens)\n",
    "        \n",
    "        results.append({\n",
    "            \"input\": input_text,\n",
    "            \"reference\": reference,\n",
    "            \"generated\": generated,\n",
    "            \"rouge1\": rouge_scores[\"rouge1\"].fmeasure,\n",
    "            \"rouge2\": rouge_scores[\"rouge2\"].fmeasure,\n",
    "            \"rougeL\": rouge_scores[\"rougeL\"].fmeasure,\n",
    "            \"bleu\": bleu_score\n",
    "        })\n",
    "        \n",
    "        # Show first few examples\n",
    "        if i < 5:\n",
    "            print(f\"Example {i+1}:\")\n",
    "            print(f\"Input: {input_text[:100]}...\")\n",
    "            print(f\"Reference: {reference[:100]}...\")\n",
    "            print(f\"Generated: {generated[:100]}...\")\n",
    "            print(\"-\" * 80)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing example {i}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56737732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame from results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Calculate average metrics\n",
    "average_metrics = {\n",
    "    \"ROUGE-1\": results_df[\"rouge1\"].mean(),\n",
    "    \"ROUGE-2\": results_df[\"rouge2\"].mean(),\n",
    "    \"ROUGE-L\": results_df[\"rougeL\"].mean(),\n",
    "    \"BLEU\": results_df[\"bleu\"].mean()\n",
    "}\n",
    "\n",
    "print(\"\\nAverage metrics:\")\n",
    "for metric, value in average_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa925a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the plot styles\n",
    "plt.style.use('ggplot')\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create bar chart of average metrics\n",
    "bars = plt.bar(\n",
    "    average_metrics.keys(), \n",
    "    average_metrics.values(),\n",
    "    color=['#ff9999', '#66b3ff', '#99ff99', '#ffcc99']\n",
    ")\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width()/2.,\n",
    "        height + 0.01,\n",
    "        f'{height:.3f}',\n",
    "        ha='center',\n",
    "        fontsize=11\n",
    "    )\n",
    "\n",
    "plt.title(\"Average Evaluation Metrics\", fontsize=15)\n",
    "plt.ylabel(\"Score\", fontsize=12)\n",
    "plt.ylim(0, max(average_metrics.values()) * 1.2)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(\"./results/evaluation_metrics.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b22ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histograms of score distributions\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "metrics = [\"rouge1\", \"rouge2\", \"rougeL\", \"bleu\"]\n",
    "titles = [\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"BLEU\"]\n",
    "\n",
    "for i, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "    row, col = i // 2, i % 2\n",
    "    axs[row, col].hist(results_df[metric], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axs[row, col].set_title(f\"{title} Score Distribution\")\n",
    "    axs[row, col].set_xlabel(\"Score\")\n",
    "    axs[row, col].set_ylabel(\"Frequency\")\n",
    "    axs[row, col].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./results/score_distributions.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde5c5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results to CSV\n",
    "os.makedirs(\"./results\", exist_ok=True)\n",
    "results_df.to_csv(\"./results/evaluation_results.csv\", index=False)\n",
    "\n",
    "# Save metrics summary\n",
    "with open(\"./results/metrics_summary.json\", \"w\") as f:\n",
    "    json.dump(average_metrics, f, indent=4)\n",
    "\n",
    "print(\"Results saved to ./results/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2982d69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample random examples for qualitative analysis\n",
    "sample_indices = np.random.choice(len(results_df), min(10, len(results_df)), replace=False)\n",
    "sample_results = results_df.iloc[sample_indices].copy()\n",
    "\n",
    "# Sort samples by ROUGE-L score to see best and worst examples\n",
    "sample_results = sample_results.sort_values(by=\"rougeL\", ascending=False)\n",
    "\n",
    "print(\"\\nQualitative Analysis - Best and Worst Examples:\")\n",
    "print(\"\\n--- TOP PERFORMING EXAMPLES ---\")\n",
    "for i, row in enumerate(sample_results.head(3).itertuples()):\n",
    "    print(f\"Example {i+1} (ROUGE-L: {row.rougeL:.4f}, BLEU: {row.bleu:.4f}):\")\n",
    "    print(f\"Input: {row.input[:150]}...\")\n",
    "    print(f\"Generated: {row.generated[:150]}...\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n--- WORST PERFORMING EXAMPLES ---\")\n",
    "for i, row in enumerate(sample_results.tail(3).itertuples()):\n",
    "    print(f\"Example {i+1} (ROUGE-L: {row.rougeL:.4f}, BLEU: {row.bleu:.4f}):\")\n",
    "    print(f\"Input: {row.input[:150]}...\")\n",
    "    print(f\"Reference: {row.reference[:150]}...\")\n",
    "    print(f\"Generated: {row.generated[:150]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4727b29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find examples with the largest discrepancy between metrics\n",
    "results_df['metric_diff'] = abs(results_df['rougeL'] - results_df['bleu'])\n",
    "interesting_cases = results_df.sort_values(by='metric_diff', ascending=False).head(5)\n",
    "\n",
    "print(\"\\nInteresting Cases (Large Metric Discrepancies):\")\n",
    "for i, row in enumerate(interesting_cases.itertuples()):\n",
    "    print(f\"Case {i+1} (ROUGE-L: {row.rougeL:.4f}, BLEU: {row.bleu:.4f}, Diff: {row.metric_diff:.4f}):\")\n",
    "    print(f\"Input: {row.input[:100]}...\")\n",
    "    print(f\"Reference: {row.reference[:100]}...\")\n",
    "    print(f\"Generated: {row.generated[:100]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
